{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q学習\n",
    "\n",
    "#### 参考にしたサイト\n",
    " - [サイト](https://github.com/matsumotokoki/RL-Maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from map import Map\n",
    "from agent import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_action(next_state, episode, q_table):\n",
    "    epsilon = 0.5\n",
    "    #εグリーディ方策\n",
    "    if epsilon <= np.random.uniform(0,1):\n",
    "        next_action = np.argmax(q_table[next_state])\n",
    "    else:\n",
    "        next_action = np.random.choice(range(4))\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q学習の式\n",
    "\n",
    "$Q(s_t, a_t) ← (1-\\alpha)Q(s_t,a_t)+\\alpha(r_{t+1} + \\gamma\\ \\mathrm{max}_{a_{t+1}}Q(s_{t+1},a_{t+1})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_update(q_table, state, action, reward, next_state):\n",
    "    #次の時刻からもらえる報酬の合計の最大値\n",
    "    next_q_max = max(q_table[next_state])\n",
    "    gamma = 0.9\n",
    "    alpha = 0.7\n",
    "    #Q学習の式\n",
    "    q_table[state, action] = (1-alpha)*q_table[state, action] + alpha*(reward + gamma * next_q_max)\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(end_or_yet, state, next_state, _map):\n",
    "    boko = []\n",
    "    for i in range(_map.shape[0]):\n",
    "        for j in range(_map.shape[1]):\n",
    "            if _map[12-j][i] == 3:\n",
    "                boko.append([12-j,i])\n",
    "\n",
    "    #座標変換\n",
    "    state_ = [state//13,state%13]\n",
    "    next_state_ = [next_state//13,next_state%13]\n",
    "\n",
    "    for boko_ in boko:\n",
    "        if state_ == boko_:\n",
    "            reward = -80\n",
    "            break\n",
    "        else:\n",
    "            reward = 1\n",
    "\n",
    "    if end_or_yet and next_state_ == [11,11]:\n",
    "        reward = 300\n",
    "    elif end_or_yet and next_state_ == [11,2]:\n",
    "        reward = 100\n",
    "    elif end_or_yet and next_state_ == [6,11]:\n",
    "        reward = 20\n",
    "    elif state == next_state:\n",
    "        reward = -10\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(reward_list, max_episode):\n",
    "    episode_list =[]\n",
    "    for i in range(max_episode):\n",
    "        num = i + 1\n",
    "        episode_list.append(num)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(episode_list, reward_list, label=\"reward_transition\") \n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    map_init = Map()\n",
    "    agent = Agent()\n",
    "    max_episode = 100\n",
    "    num_step = 300\n",
    "    q_table = np.random.uniform(low=-1, high=1, size=(map_init.size**2, agent.action_space))\n",
    "    reward_list = []\n",
    "\n",
    "    for episode in range(max_episode):\n",
    "        agent = Agent(map_init.init_pos)\n",
    "        state = agent.get_state()\n",
    "        choice_action = np.argmax(q_table[state])\n",
    "        count = 0\n",
    "\n",
    "        #step\n",
    "        for i in range(num_step):\n",
    "            direction = map_init.check_move(agent.pos)\n",
    "            agent.action(choice_action, direction)\n",
    "            end_or_yet = agent.check_done()\n",
    "            next_state = agent.get_state()\n",
    "            get_reward = reward(end_or_yet, state, next_state, map_init.map)\n",
    "            count += get_reward\n",
    "            q_table = q_update(q_table, state, choice_action, get_reward, next_state)\n",
    "            choice_action = decide_action(next_state, episode, q_table)\n",
    "            state = next_state\n",
    "            map_init.plot(agent.pos, q_table)\n",
    "            if end_or_yet:\n",
    "                break\n",
    "        reward_list.append(count)\n",
    "        print(\"episode %5d, reward %6d, step %5d\" %(episode+1,count,i+1))\n",
    "    graph(reward_list, max_episode)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
